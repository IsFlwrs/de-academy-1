{"paragraphs":[{"text":"%md\n#  Spark Optimization Scenarios\nWelcome to the sixth session notebook. You’re very close to finishing the Wizeline Certification for Big Data Engineering with Spark!\nIf you have any feedback about our courses, email us at academy@wizeline.com or use the Academy Slack channel.","user":"anonymous","dateUpdated":"2018-08-30T15:13:21+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Spark Optimization Scenarios</h1>\n<p>Welcome to the sixth session notebook. You’re very close to finishing the Wizeline Certification for Big Data Engineering with Spark!<br/>If you have any feedback about our courses, email us at <a href=\"mailto:&#97;c&#97;&#x64;&#x65;&#x6d;&#x79;&#64;&#x77;&#x69;z&#101;&#x6c;&#105;n&#101;&#46;&#99;o&#109;\">&#97;c&#97;&#x64;&#x65;&#x6d;&#x79;&#64;&#x77;&#x69;z&#101;&#x6c;&#105;n&#101;&#46;&#99;o&#109;</a> or use the Academy Slack channel.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963772_-433931036","id":"20180829-222909_1145138508","dateCreated":"2018-08-30T15:12:43+0000","dateStarted":"2018-08-30T15:13:21+0000","dateFinished":"2018-08-30T15:13:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:184"},{"text":"%md\n# About this Notebook\nToday there are no exercises on this notebook.\n \nYou will find code that solves a specific request and was later optimized with Spark features. Scala and PySpark versions are intercalated, each with or without `%pyspark` header. \n \nWe encourage you to read through the code explanations. Feel free to experiment and try to further optimize the code pieces. Remember, there is always room for improvement.","dateUpdated":"2018-08-30T15:12:43+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>About this Notebook</h1>\n<p>Today there are no exercises on this notebook.</p>\n<p>You will find code that solves a specific request and was later optimized with Spark features. Scala and PySpark versions are intercalated, each with or without <code>%pyspark</code> header. </p>\n<p>We encourage you to read through the code explanations. Feel free to experiment and try to further optimize the code pieces. Remember, there is always room for improvement.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963786_-351594771","id":"20180829-223008_1634680293","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:185"},{"text":"%md\n# Before We Begin\nLet’s only load up the required libraries where they make sense. We do not want to measure that as part of each cell execution.","dateUpdated":"2018-08-30T15:12:43+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Before We Begin</h1>\n<p>Let’s only load up the required libraries where they make sense. We do not want to measure that as part of each cell execution.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963790_-353133767","id":"20180829-223052_2122433210","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186"},{"text":"%pyspark\nimport pyspark.sql.functions as fun\nfrom pyspark.sql.functions import month\nfrom operator import add\nfrom pyspark.sql import Row\n\ndef checkPartitions(df):\n    print(\"Number of partitions used: {}\".format(df.rdd.getNumPartitions()))\n    print(\"Sample of objects per partitions\")\n    partitions = df.rdd.mapPartitions(lambda part: [Row(value=len(list(part)))])\n    spark.createDataFrame(partitions).show()","dateUpdated":"2018-08-30T15:12:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1535641963793_-343515045","id":"20180829-223101_538753799","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:187"},{"text":"def checkPartitions(df: org.apache.spark.sql.DataFrame) = {\n    println(\"Number of partitions used: \" + df.rdd.getNumPartitions)\n    println(\"Sample of objects per partitions\")\n    df.mapPartitions(iter => Seq(iter.size).iterator).show\n}\n","dateUpdated":"2018-08-30T15:12:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"checkPartitions: (df: org.apache.spark.sql.DataFrame)Unit\n"}]},"apps":[],"jobName":"paragraph_1535641963795_-342745547","id":"20180829-223307_2068370091","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:188"},{"text":"%md Let’s also load up the base file.","dateUpdated":"2018-08-30T15:12:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Let’s also load up the base file.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963796_-344669291","id":"20180829-223526_208873282","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:189"},{"text":"%pyspark\norders = spark.read.json(\"gs://de-training-input/alimazon/200000/client-orders/\").cache()\norders_sample = orders.sample(False, 0.25).cache()\n","dateUpdated":"2018-08-30T17:44:09+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1535641963796_-344669291","id":"20180829-223536_1861014599","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:190"},{"text":"val orders  = spark.read.json(\"gs://de-training-input/alimazon/200000/client-orders/\").cache\nval ordersSample = orders.sample(false, 0.25).cache()","dateUpdated":"2018-08-30T17:44:39+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963796_-344669291","id":"20180829-223544_1641843554","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:191"},{"text":"%md\n**NOTE:** For a clearer benchmark, cache is used at several points in this notebook.\n","user":"anonymous","dateUpdated":"2018-08-30T15:12:59+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>NOTE:</strong> For a clearer benchmark, cache is used at several points in this notebook.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963796_-344669291","id":"20180829-223602_63436214","dateCreated":"2018-08-30T15:12:43+0000","dateStarted":"2018-08-30T15:12:59+0000","dateFinished":"2018-08-30T15:13:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:192"},{"text":"%md\n# Live Code 1 - Catalyst Advantages\n**Code Purpose**\nThe Alimazon marketing team wants to run a digital ads campaign on their website. They intend to target each client with ads related to the items they spend the most money on.  You will deliver a report with the amount of money spent by each user on every product they have ever purchased. Also, sort the report by users and total amount spent per unique product.\n\nYour first approach was to use RDDs. As a good practice, you print only the first 20 rows to review your own results before creating the full report. You made good use of the Map operations and RDD properties for the task at hand.\n","dateUpdated":"2018-08-30T15:12:43+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Live Code 1 - Catalyst Advantages</h1>\n<p><strong>Code Purpose</strong><br/>The Alimazon marketing team wants to run a digital ads campaign on their website. They intend to target each client with ads related to the items they spend the most money on. You will deliver a report with the amount of money spent by each user on every product they have ever purchased. Also, sort the report by users and total amount spent per unique product.</p>\n<p>Your first approach was to use RDDs. As a good practice, you print only the first 20 rows to review your own results before creating the full report. You made good use of the Map operations and RDD properties for the task at hand.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963797_-345054040","id":"20180829-223629_1747329333","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:193"},{"text":"%pyspark\nordersRDD = orders_sample.rdd\nsorted_rows = ordersRDD.map(lambda row: ((row[0], row[2]), row[5])) \\\n                       .groupByKey() \\\n                       .map(lambda row: (row[0], sum(row[1]))) \\\n                       .sortBy(lambda row: (row[0][0], row[1]), ascending = False) \\\n                       .take(20)\nprint('Sorted rows:')\nfor row in sorted_rows:\n    print(row)","dateUpdated":"2018-08-30T17:45:04+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963797_-345054040","id":"20180829-223904_1848044529","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:194"},{"text":"val ordersRDD = ordersSample.rdd\nval sortedRows = ordersRDD.map(row => ((row.getString(0), row.getString(2)), row.getDouble(5)))\n                      .groupByKey.map{case (key, iter) => (key, iter.sum)}\n                      .sortBy(row => (row._1._1,  row._2), ascending = false)\n                      .take(20)\nfor (row <- sortedRows) {\n    println(row)\n}","dateUpdated":"2018-08-30T17:45:08+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963800_-346208287","id":"20180829-223922_1652280103","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:195"},{"text":"%md\nThe code seems to produce the correct results. However, before running the saveAsTextFile method, you notice how long it took to run with a small sample. \n\nTo optimize this run time, remember that `Spark 2.2.0` (the version you’re using) already has Catalyst optimizations with DataFrames. So, you change your code accordingly. In essence, if your software has implemented optimizations for you, it’s a good idea to take advantage of them.","dateUpdated":"2018-08-30T15:12:43+0000","config":{"tableHide":false,"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The code seems to produce the correct results. However, before running the saveAsTextFile method, you notice how long it took to run with a small sample. </p>\n<p>To optimize this run time, remember that <code>Spark 2.2.0</code> (the version you’re using) already has Catalyst optimizations with DataFrames. So, you change your code accordingly. In essence, if your software has implemented optimizations for you, it’s a good idea to take advantage of them.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963800_-346208287","id":"20180829-223838_2106718386","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:196"},{"text":"%pyspark\n# Optimal Scenario - Using DataFrames\norders_sample.groupBy(fun.col(\"client_id\"), fun.col(\"product_id\")) \\\n      .agg(fun.sum(\"total\").alias(\"gross-sales\")) \\\n      .orderBy(fun.desc(\"client_id\"), fun.desc(\"gross-sales\")) \\\n      .show()","dateUpdated":"2018-08-30T17:46:15+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963800_-346208287","id":"20180829-224950_201420839","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:197"},{"text":"// Optimal Scenario - Using DataFrames\nordersSample.groupBy(col(\"client_id\"), col(\"product_id\"))\n      .agg(sum(\"total\").alias(\"gross-sales\"))\n      .orderBy(desc(\"client_id\"), desc(\"gross-sales\"))\n      .show\n","dateUpdated":"2018-08-30T17:46:24+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963801_-346593036","id":"20180829-225010_174005329","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:198"},{"text":"%md\nNow that the execution time has been reduced greatly, you can see how your final implementation takes advantage of Catalyst. Although both codes do approximately the same task, the final one has been optimized on the backend by Spark.\n\nAn alternative version through the RDD route would be using reduceByKey or aggByKey. According to the documentation, either of these can reduce the latency when working with RDDs by first reducing locally, then shuffling, and finally reducing again.","dateUpdated":"2018-08-30T15:12:43+0000","config":{"editorSetting":{"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now that the execution time has been reduced greatly, you can see how your final implementation takes advantage of Catalyst. Although both codes do approximately the same task, the final one has been optimized on the backend by Spark.</p>\n<p>An alternative version through the RDD route would be using reduceByKey or aggByKey. According to the documentation, either of these can reduce the latency when working with RDDs by first reducing locally, then shuffling, and finally reducing again.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963805_-348132031","id":"20180829-224057_45519513","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:199"},{"text":"%pyspark\n# Opimization Using reduceByKey\nsorted_rows = ordersRDD.map(lambda row: ((row[0], row[2]), row[5])) \\\n                       .reduceByKey(add) \\\n                       .sortBy(lambda row: (row[0][0], row[1]), ascending = False) \\\n                       .take(20)\nprint('Sorted rows:')\nfor row in sorted_rows:\n    print(row)","dateUpdated":"2018-08-30T15:12:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963806_-346977785","id":"20180829-192512_1773196375","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:200"},{"text":"// Optimization Using reduceByKey\nval sortedRows = ordersRDD.map(row => ((row.getString(0), row.getString(2)), row.getDouble(5)))\n                          .reduceByKey(_+_)\n                          .sortBy(pair => (pair._1._1,  pair._2), ascending = false)\n                          .take(20)\nfor (row <- sortedRows) {\n    println(row)\n}\n","dateUpdated":"2018-08-30T15:12:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963807_-347362534","id":"20180829-210413_306858813","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:201"},{"text":"%md Does the benchmark result makes sense? Which one would you select and why? What would happen with even more data? What else could you do to keep improving this code piece?","dateUpdated":"2018-08-30T15:12:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Does the benchmark result makes sense? Which one would you select and why? What would happen with even more data? What else could you do to keep improving this code piece?</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963807_-347362534","id":"20180829-224121_622882701","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:202"},{"text":"%md\n# Live Code 2 - Optimize with Repartition and Cache\nThe marketing campaign was a success! Even the marketing VP took an interest oin your work with Spark and met directly with you to present his latest idea. To extract more utility from the Alimazon dataset, he would like to study how users spend their money. \n\nYour task, should you choose to accept it, is to compute the SUM, MEAN, MAX, and MIN of the money spent by each user on the Alimazon orders dataset. This sounds quite easy, considering the previous request.\n\nFor some reasons, the Marketing VP also wants the metrics on separate files. You go ahead and create the following code.\n\n","dateUpdated":"2018-08-30T15:12:43+0000","config":{"editorSetting":{"language":"text","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/text","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Live Code 2 - Optimize with Repartition and Cache</h1>\n<p>The marketing campaign was a success! Even the marketing VP took an interest oin your work with Spark and met directly with you to present his latest idea. To extract more utility from the Alimazon dataset, he would like to study how users spend their money. </p>\n<p>Your task, should you choose to accept it, is to compute the SUM, MEAN, MAX, and MIN of the money spent by each user on the Alimazon orders dataset. This sounds quite easy, considering the previous request.</p>\n<p>For some reasons, the Marketing VP also wants the metrics on separate files. You go ahead and create the following code.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963808_-361598243","id":"20180829-225242_1534870793","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:203"},{"text":"%pyspark\n# Base Scenario - Data as read by spark\norders.show()\norders.groupBy(\"client_id\").sum(\"total\").show()\norders.groupBy(\"client_id\").avg(\"total\").show()\norders.groupBy(\"client_id\").min(\"total\").show()\norders.groupBy(\"client_id\").max(\"total\").show()\n","dateUpdated":"2018-08-30T15:12:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963808_-361598243","id":"20180829-225331_739320534","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:204"},{"text":"// Base Scenario - Data as read by spark\norders.show\norders.groupBy(\"client_id\").sum(\"total\").show\norders.groupBy(\"client_id\").avg(\"total\").show\norders.groupBy(\"client_id\").min(\"total\").show\norders.groupBy(\"client_id\").max(\"total\").show\n","dateUpdated":"2018-08-30T15:12:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963810_-360828745","id":"20180829-225337_785829570","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:205"},{"text":"%md\nThis was fairly easy compared to the previous task. Yet, being the efficiency-hungry developer you are, you think there is a way to make this code work faster. You go with the repartition method, because it makes sense to order data with the column you will use later. You expect the shuffling and latency to be reduced significantly.","dateUpdated":"2018-08-30T15:12:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>This was fairly easy compared to the previous task. Yet, being the efficiency-hungry developer you are, you think there is a way to make this code work faster. You go with the repartition method, because it makes sense to order data with the column you will use later. You expect the shuffling and latency to be reduced significantly.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963810_-360828745","id":"20180829-225501_317647165","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:206"},{"text":"%pyspark\n# Using checkPartition function\norders_rep   =  orders.repartition(\"client_id\")\norders_rep.show\ncheckPartitions(orders)\ncheckPartitions(orders_rep)","dateUpdated":"2018-08-30T17:49:16+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963812_-363137238","id":"20180829-225743_1301521202","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:210","focus":true},{"text":"// Using checkPartition function\nval orders_rep   =  orders.repartition($\"client_id\")\norders_rep.show\ncheckPartitions(orders)\ncheckPartitions(orders_rep)","dateUpdated":"2018-08-30T17:49:21+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963812_-363137238","id":"20180829-225754_1372456767","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:211","focus":true},{"text":"%pyspark\n# Failed \"Improvement\" Scenario - Repartitioned data\norders_rep.groupBy(\"client_id\").sum(\"total\").show()\norders_rep.groupBy(\"client_id\").avg(\"total\").show()\norders_rep.groupBy(\"client_id\").min(\"total\").show()\norders_rep.groupBy(\"client_id\").max(\"total\").show()","dateUpdated":"2018-08-30T17:49:29+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963811_-361213494","id":"20180829-225523_676296318","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:207"},{"text":"// Failed \"Improvement\" Scenario - Repartitioned data\norders_rep.groupBy(\"client_id\").sum(\"total\").show\norders_rep.groupBy(\"client_id\").avg(\"total\").show\norders_rep.groupBy(\"client_id\").min(\"total\").show\norders_rep.groupBy(\"client_id\").max(\"total\").show","dateUpdated":"2018-08-30T17:49:31+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963812_-363137238","id":"20180829-225532_334396161","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:208"},{"text":"%md\nWeirdly, performance does not improve, as you were expecting it to. You use a quick function to monitor the partitions. You think something might have gone wrong with the partitioning that you can detect.\n\nAfter reviewing the results, you notice that partitions are indeed different, but there isn’t anything obvious to optimize there. Repartition did it’s work! You decide to run an explain.","dateUpdated":"2018-08-30T17:50:07+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Weirdly, performance does not improve, as you were expecting it to. You use a quick function to monitor the partitions. You think something might have gone wrong with the partitioning that you can detect.</p>\n<p>After reviewing the results, you notice that partitions are indeed different, but there isn’t anything obvious to optimize there. Repartition did it’s work! You decide to run an explain.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963812_-363137238","id":"20180829-225601_437916405","dateCreated":"2018-08-30T15:12:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:209","user":"anonymous","dateFinished":"2018-08-30T17:50:07+0000","dateStarted":"2018-08-30T17:50:07+0000"},{"text":"%pyspark\norders.explain()\norders_rep.explain()","dateUpdated":"2018-08-30T15:15:20+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963813_-363521987","id":"20180829-230236_608426328","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:213"},{"text":"orders.explain\norders_rep.explain","dateUpdated":"2018-08-30T17:50:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1535641963813_-363521987","id":"20180829-230249_1160763117","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:214"},{"text":"%md There it is. The shuffling still had to occur on each call to show. Going back to the documentation, you realize you might have made a mistake on the cache location. Each action executes the full DAG taking cache into account, which means the repartition method happened again on every action. A quick rewrite solves it.\n","dateUpdated":"2018-08-30T15:15:42+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>There it is. The shuffling still had to occur on each call to show. Going back to the documentation, you realize you might have made a mistake on the cache location. Each action executes the full DAG taking cache into account, which means the repartition method happened again on every action. A quick rewrite solves it.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963813_-363521987","id":"20180829-230307_51822834","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:215"},{"text":"%pyspark\n# Optimal Scenario - Repartitioned data\norders_rep_cached  = orders.repartition(\"client_id\").cache()\norders_rep_cached.show\ncheckPartitions(orders_rep_cached)\norders_rep_cached.groupBy(\"client_id\").sum(\"total\").show()\norders_rep_cached.groupBy(\"client_id\").avg(\"total\").show()\norders_rep_cached.groupBy(\"client_id\").min(\"total\").show()\norders_rep_cached.groupBy(\"client_id\").max(\"total\").show()\n\n","dateUpdated":"2018-08-30T15:12:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963813_-363521987","id":"20180829-214700_1621651268","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216"},{"text":"// Optimal Scenario - Repartitioned data\nval orders_rep_cached  = orders.repartition($\"client_id\").cache\norders_rep_cached.show\ncheckPartitions(orders_rep_cached)\norders_rep_cached.groupBy(\"client_id\").sum(\"total\").show\norders_rep_cached.groupBy(\"client_id\").avg(\"total\").show\norders_rep_cached.groupBy(\"client_id\").min(\"total\").show\norders_rep_cached.groupBy(\"client_id\").max(\"total\").show","dateUpdated":"2018-08-30T15:12:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963813_-363521987","id":"20180829-192240_1457079661","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:217"},{"text":"%md\nJust moving the cache solved your performance, because caching stores a point in your execution DAG in the selected storage level, until the next action requires it. This way, the correct usage of repartition and cache made the difference in execution time. ","dateUpdated":"2018-08-30T15:12:43+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Just moving the cache solved your performance, because caching stores a point in your execution DAG in the selected storage level, until the next action requires it. This way, the correct usage of repartition and cache made the difference in execution time.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963814_-362367741","id":"20180829-230337_936315368","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:218"},{"text":"%md\n# Live Code 3 - Optimize with Repartition\nNow that you’re moving up in the corporate ladder using Spark (because no one else knows how to use it in your organization), it is time to make some suggestions to make yourself more noticeable. You decide to push for an architectural change that will improve performance in your already generated reports. For this purpose, you want to experiment with repartition.\n\nYou know that the Alimazon storage is configured with monthly partitions. That does not seem optimal to you, but you need to generate some evidence to support your thoughts. You decide to run a few experimental partitions to observe if they are good for several shuffling scenarios.\n\n**NOTE:** Because of a limitation of Spark reading from GCS, we will have to simulate the partition by month that Alimazon uses using the following code.\n","user":"anonymous","dateUpdated":"2018-08-30T17:54:35+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Live Code 3 - Optimize with Repartition</h1>\n<p>Now that you’re moving up in the corporate ladder using Spark (because no one else knows how to use it in your organization), it is time to make some suggestions to make yourself more noticeable. You decide to push for an architectural change that will improve performance in your already generated reports. For this purpose, you want to experiment with repartition.</p>\n<p>You know that the Alimazon storage is configured with monthly partitions. That does not seem optimal to you, but you need to generate some evidence to support your thoughts. You decide to run a few experimental partitions to observe if they are good for several shuffling scenarios.</p>\n<p><strong>NOTE:</strong> Because of a limitation of Spark reading from GCS, we will have to simulate the partition by month that Alimazon uses using the following code.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535643040926_-1084211396","id":"20180830-153040_1949700509","dateCreated":"2018-08-30T15:30:40+0000","dateStarted":"2018-08-30T17:54:35+0000","dateFinished":"2018-08-30T17:54:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:219"},{"text":"%pyspark\nordersByMonth = orders.repartition(12, month(\"timestamp\").alias(\"month\")).cache()\nordersByMonth.show()","user":"anonymous","dateUpdated":"2018-08-30T15:31:49+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535643097989_1804006652","id":"20180830-153137_747080515","dateCreated":"2018-08-30T15:31:37+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:220"},{"text":"val ordersByMonth = orders.repartition(12, month($\"timestamp\").alias(\"month\")).cache\nordersByMonth.show","user":"anonymous","dateUpdated":"2018-08-30T17:55:04+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535643183948_-1269044110","id":"20180830-153303_1383442079","dateCreated":"2018-08-30T15:33:03+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:221"},{"text":"%md\nFor the sake of simplicity, you decide to start testing with the same report metrics requested by the marketing VP. On this test, you want to obtain only the metrics for February, because that will reduce the computation time for the benchmark. More tests with distinct shuffling implications should be done, but this is good enough to start.\n\nYou run your partition analysis function first to study how the data is partitioned right now.\n","user":"anonymous","dateUpdated":"2018-08-30T15:33:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>For the sake of simplicity, you decide to start testing with the same report metrics requested by the marketing VP. On this test, you want to obtain only the metrics for February, because that will reduce the computation time for the benchmark. More tests with distinct shuffling implications should be done, but this is good enough to start.</p>\n<p>You run your partition analysis function first to study how the data is partitioned right now.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535643206408_1398465987","id":"20180830-153326_908443610","dateCreated":"2018-08-30T15:33:26+0000","dateStarted":"2018-08-30T15:33:33+0000","dateFinished":"2018-08-30T15:33:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:222"},{"text":"%pyspark\ncheckPartitions(ordersByMonth)","user":"anonymous","dateUpdated":"2018-08-30T15:33:48+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535643224283_-658234711","id":"20180830-153344_2081551598","dateCreated":"2018-08-30T15:33:44+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:223"},{"text":"checkPartitions(ordersByMonth)","user":"anonymous","dateUpdated":"2018-08-30T15:33:56+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535643228716_675689725","id":"20180830-153348_516778534","dateCreated":"2018-08-30T15:33:48+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:224"},{"text":"%md After checking the partitions, let’s run the base benchmark to measure the rest of your experiments. When benchmarking your code (or any optimizations) it is always wise to have a base scenario with minimal or no optimizations at all.\n","user":"anonymous","dateUpdated":"2018-08-30T15:34:15+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>After checking the partitions, let’s run the base benchmark to measure the rest of your experiments. When benchmarking your code (or any optimizations) it is always wise to have a base scenario with minimal or no optimizations at all.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535643247772_1611829772","id":"20180830-153407_141959711","dateCreated":"2018-08-30T15:34:07+0000","dateStarted":"2018-08-30T15:34:15+0000","dateFinished":"2018-08-30T15:34:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:225"},{"text":"%pyspark\n# Base scenario: multiple metrics for february\nordersInFebruary = ordersByMonth.filter(month(\"timestamp\") == \"2\")\n\n# Exploring base scenario partitioning\nordersInFebruary.show()\ncheckPartitions(ordersInFebruary)","user":"anonymous","dateUpdated":"2018-08-30T17:56:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535651740051_-2086471964","id":"20180830-175540_1781171772","dateCreated":"2018-08-30T17:55:40+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1875"},{"text":"// Base scenario: multiple metrics for february\nval ordersInFebruary = ordersByMonth.filter(month($\"timestamp\") === \"2\")\n\n// Exploring base scenario partitioning\nordersInFebruary.show\ncheckPartitions(ordersinFebruary)","user":"anonymous","dateUpdated":"2018-08-30T17:57:32+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"text","editOnDblClick":false},"editorMode":"ace/mode/text"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535651783101_1280758465","id":"20180830-175623_2069300436","dateCreated":"2018-08-30T17:56:23+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1931"},{"text":"%pyspark\n# Base scenario: multiple metrics for february\nordersInFebruary.groupBy(month(\"timestamp\")).sum(\"total\").show()\nordersInFebruary.groupBy(month(\"timestamp\")).avg(\"total\").show()\nordersInFebruary.groupBy(month(\"timestamp\")).min(\"total\").show()\nordersInFebruary.groupBy(month(\"timestamp\")).max(\"total\").show()\n","user":"anonymous","dateUpdated":"2018-08-30T17:56:22+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535643345824_1616016037","id":"20180830-153545_1864356302","dateCreated":"2018-08-30T15:35:45+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:226"},{"text":"// Scala version\nordersInFebruary.groupBy(month($\"timestamp\")).sum(\"total\").show\nordersInFebruary.groupBy(month($\"timestamp\")).avg(\"total\").show\nordersInFebruary.groupBy(month($\"timestamp\")).min(\"total\").show\nordersInFebruary.groupBy(month($\"timestamp\")).max(\"total\").show","user":"anonymous","dateUpdated":"2018-08-30T17:57:52+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535643529500_-453134544","id":"20180830-153849_2103987845","dateCreated":"2018-08-30T15:38:49+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:227"},{"text":"%md Cache is not used directly on ordersInFebruary because although filter requires some compute time, it is optimized on Spark. Even more so, because it only has narrow dependencies. The results are fast, but that’s mostly because we dropped all months other than February so there were less partitions to work with in the nodes. \n\nNow you can proceed with your first repartition experiment. You want to avoid repartitioning by client_id, because you want to have something that will work for any shuffling scenario. You decide to start with an increased number of partitions on the month of February as a first approach. \n","user":"anonymous","dateUpdated":"2018-08-30T15:39:52+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Cache is not used directly on ordersInFebruary because although filter requires some compute time, it is optimized on Spark. Even more so, because it only has narrow dependencies. The results are fast, but that’s mostly because we dropped all months other than February so there were less partitions to work with in the nodes. </p>\n<p>Now you can proceed with your first repartition experiment. You want to avoid repartitioning by client_id, because you want to have something that will work for any shuffling scenario. You decide to start with an increased number of partitions on the month of February as a first approach.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535643586421_730645877","id":"20180830-153946_1575142577","dateCreated":"2018-08-30T15:39:46+0000","dateStarted":"2018-08-30T15:39:52+0000","dateFinished":"2018-08-30T15:39:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:228"},{"text":"%pyspark\n# Bad optimization scenario\nordersInFebruary = ordersByMonth.filter(month(\"timestamp\") == \"2\")\nrepartitionedOrdersInFebruary1 = ordersInFebruary.repartition(3000).cache()\ncheckPartitions(ordersInFebruary)\n","user":"anonymous","dateUpdated":"2018-08-30T17:58:22+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535643668531_-525635903","id":"20180830-154108_122475076","dateCreated":"2018-08-30T15:41:08+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:229"},{"text":"// Bad optimization scenario\nval ordersInFebruary = ordersByMonth.filter(month($\"timestamp\") === \"2\")\nval repartitionedOrdersInFebruary1 = ordersInFebruary.repartition(3000).cache\ncheckPartitions(repartitionedOrdersInFebruary1)\n","user":"anonymous","dateUpdated":"2018-08-30T17:58:31+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535643711838_655035878","id":"20180830-154151_443575427","dateCreated":"2018-08-30T15:41:51+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:230"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535651902507_-940660735","id":"20180830-175822_56277178","dateCreated":"2018-08-30T17:58:22+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2006","text":"%pyspark\nrepartitionedOrdersInFebruary1.groupBy(month(\"timestamp\")).sum(\"total\").show()\nrepartitionedOrdersInFebruary1.groupBy(month(\"timestamp\")).avg(\"total\").show()\nrepartitionedOrdersInFebruary1.groupBy(month(\"timestamp\")).min(\"total\").show()\nrepartitionedOrdersInFebruary1.groupBy(month(\"timestamp\")).max(\"total\").show()","dateUpdated":"2018-08-30T17:58:28+0000"},{"text":"// Scala Version\nrepartitionedOrdersInFebruary1.groupBy(month($\"timestamp\")).sum(\"total\").show\nrepartitionedOrdersInFebruary1.groupBy(month($\"timestamp\")).avg(\"total\").show\nrepartitionedOrdersInFebruary1.groupBy(month($\"timestamp\")).min(\"total\").show\nrepartitionedOrdersInFebruary1.groupBy(month($\"timestamp\")).max(\"total\").show","user":"anonymous","dateUpdated":"2018-08-30T17:58:43+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535651912021_1477501426","id":"20180830-175832_714783056","dateCreated":"2018-08-30T17:58:32+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2067"},{"text":"%md\nPerformance could certainly improve. This is a proof that just adding more partitions are not directly linked to better performance. As you can see in the checkPartitions output, plenty of partitions are empty, which is not good because it translates into useless computing time used by the worker nodes.\n\nAlso, remember that we’re working on a relatively small Spark cluster, which limits the amount of CPUs we have access to. Considering that, let’s say we want to partition to something more manageable.\n\nLet’s repartition again separately (to have comparable benchmark results).\n","user":"anonymous","dateUpdated":"2018-08-30T15:42:35+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Performance could certainly improve. This is a proof that just adding more partitions are not directly linked to better performance. As you can see in the checkPartitions output, plenty of partitions are empty, which is not good because it translates into useless computing time used by the worker nodes.</p>\n<p>Also, remember that we’re working on a relatively small Spark cluster, which limits the amount of CPUs we have access to. Considering that, let’s say we want to partition to something more manageable.</p>\n<p>Let’s repartition again separately (to have comparable benchmark results).</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535643750636_830497224","id":"20180830-154230_1501626694","dateCreated":"2018-08-30T15:42:30+0000","dateStarted":"2018-08-30T15:42:35+0000","dateFinished":"2018-08-30T15:42:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:231"},{"text":"%pyspark\nrepartitionedOrdersInFebruary2 = ordersInFebruary.repartition(9).cache()\ncheckPartitions(repartitionedOrdersInFebruary2)\n","user":"anonymous","dateUpdated":"2018-08-30T15:43:06+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535643774920_-1301750532","id":"20180830-154254_213443068","dateCreated":"2018-08-30T15:42:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:232"},{"text":"repartitionedOrdersInFebruary2 = ordersInFebruary.repartition(9).cache\ncheckPartitions(repartitionedOrdersInFebruary2)","user":"anonymous","dateUpdated":"2018-08-30T15:43:19+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535643786824_1269541692","id":"20180830-154306_2010352408","dateCreated":"2018-08-30T15:43:06+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:233"},{"text":"%md The output of checkPartitions no longer have empty partitions. Let’s see how our benchmark code behaves with these bigger sets.","user":"anonymous","dateUpdated":"2018-08-30T15:45:06+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The output of checkPartitions no longer have empty partitions. Let’s see how our benchmark code behaves with these bigger sets.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535643840056_-1985926021","id":"20180830-154400_1865638795","dateCreated":"2018-08-30T15:44:00+0000","dateStarted":"2018-08-30T15:45:06+0000","dateFinished":"2018-08-30T15:45:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:234"},{"text":"%pyspark\n# Optimal solution\nrepartitionedOrdersInFebruary2.groupBy(month(\"timestamp\")).sum(\"total\").show()\nrepartitionedOrdersInFebruary2.groupBy(month(\"timestamp\")).avg(\"total\").show()\nrepartitionedOrdersInFebruary2.groupBy(month(\"timestamp\")).min(\"total\").show()\nrepartitionedOrdersInFebruary2.groupBy(month(\"timestamp\")).max(\"total\").show()","dateUpdated":"2018-08-30T15:45:28+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963814_-362367741","id":"20180829-220518_1718371005","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:235"},{"text":"// Optimal solution\n// Scala Version\nrepartitionedOrdersInFebruary2.groupBy(month($\"timestamp\")).sum(\"total\").show\nrepartitionedOrdersInFebruary2.groupBy(month($\"timestamp\")).avg(\"total\").show\nrepartitionedOrdersInFebruary2.groupBy(month($\"timestamp\")).min(\"total\").show\nrepartitionedOrdersInFebruary2.groupBy(month($\"timestamp\")).max(\"total\").show","dateUpdated":"2018-08-30T15:45:33+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535641963814_-362367741","id":"20180829-192450_551330582","dateCreated":"2018-08-30T15:12:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236"},{"text":"%md\nYou see that performance significantly improved compared to the other scenarios. Even though partitions contain a larger amount of records, we must be aware of two things:\n 1) Due to a Data-Proc configuration, we can use 3 CPUs in our cluster.\n 2) General rule-of-thumb for partitioning suggest 3 partitions per CPU. This allows the tasks to be scheduled more fairly between executor threads. However, this may not be efficient for all scenarios.\n","user":"anonymous","dateUpdated":"2018-08-30T15:46:27+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>You see that performance significantly improved compared to the other scenarios. Even though partitions contain a larger amount of records, we must be aware of two things:<br/> 1) Due to a Data-Proc configuration, we can use 3 CPUs in our cluster.<br/> 2) General rule-of-thumb for partitioning suggest 3 partitions per CPU. This allows the tasks to be scheduled more fairly between executor threads. However, this may not be efficient for all scenarios.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535641963820_-366215230","id":"20180829-230512_1037389140","dateCreated":"2018-08-30T15:12:43+0000","dateStarted":"2018-08-30T15:46:27+0000","dateFinished":"2018-08-30T15:46:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:237"},{"text":"","user":"anonymous","dateUpdated":"2018-08-30T15:47:17+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535643954739_-1083504316","id":"20180830-154554_116265660","dateCreated":"2018-08-30T15:45:54+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:238"}],"name":"C6 Notebook Spark Optimization","id":"2DQJ734ZY","angularObjects":{"2DRD82D5R:shared_process":[],"2DP272NW1:shared_process":[],"2DNNW46TD:shared_process":[],"2DPU5NUBF:shared_process":[],"2DPNSBZ5S:shared_process":[],"2DN7QV1SN:shared_process":[],"2DS4VAK6Q:shared_process":[],"2DNTMD1ZM:shared_process":[],"2DNHUTWJD:shared_process":[],"2DNX1FJ5S:shared_process":[],"2DPV16NX7:shared_process":[],"2DR19S4EA:shared_process":[],"2DNVXTJPD:shared_process":[],"2DR2D725T:shared_process":[],"2DNWBARW3:shared_process":[],"2DQEHZ71C:shared_process":[],"2DR6T4VF5:shared_process":[],"2DP6SCY3W:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}
