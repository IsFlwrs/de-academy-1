{"paragraphs":[{"text":"%md\n# Spark Data Partitioning\n\nWelcome to the notebook with the exercises for the fifth session. You’re halfway on the path to obtain the Wizeline Certification for Big Data Engineering with Spark!\nIf you have any feedback about our courses, email us at academy@wizeline.com or use the Academy Slack channel.\n \nIn this notebook you will be doing four exercises aimed at:\n - Looking at the data distribution of the partitions stored in your cluster.\n - Use different partitioning approaches to understand which is the best approach.\n - Understand the impact of the partitioning on the I/O functions.\n","user":"anonymous","dateUpdated":"2018-08-27T21:18:48+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.NullPointerException\n\tat org.apache.zeppelin.interpreter.InterpreterOutput.write(InterpreterOutput.java:287)\n\tat org.apache.zeppelin.interpreter.InterpreterResult.add(InterpreterResult.java:85)\n\tat org.apache.zeppelin.interpreter.InterpreterResult.<init>(InterpreterResult.java:70)\n\tat org.apache.zeppelin.markdown.Markdown.interpret(Markdown.java:100)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:97)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:498)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:175)\n\tat org.apache.zeppelin.scheduler.ParallelScheduler$JobRunner.run(ParallelScheduler.java:162)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n"}]},"apps":[],"jobName":"paragraph_1535404705335_-1452577299","id":"20180824-154704_999864938","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:48+0000","dateFinished":"2018-08-27T21:18:51+0000","status":"ERROR","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:253"},{"title":"","text":"%md\n# Exercise 1- Partitions and data distribution\n\nIn this exercise, you will look at the the data contained in different partitions. Pay attention to how the data is distributed between the partitions depending on how they are created.\n\nYou are expected to use the DataSet sample `transactions`, which contains monetary transactions for users in the world.\n","user":"anonymous","dateUpdated":"2018-08-27T21:18:48+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Exercise 1- Partitions and data distribution</h1>\n<p>In this exercise, you will look at the the data contained in different partitions. Pay attention to how the data is distributed between the partitions depending on how they are created.</p>\n<p>You are expected to use the DataSet sample <code>transactions</code>, which contains monetary transactions for users in the world.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705343_-1455655290","id":"20180824-160322_654860629","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:51+0000","dateFinished":"2018-08-27T21:18:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:254"},{"text":"%pyspark\n\nTransactionSchema = ['name', 'amount', 'country']\n\ntransactions = [\n    ('Bob', 100, 'United Kingdom'),\n    ('James', 15, 'United Kingdom'),\n    ('Marek', 51, 'Poland'),\n    ('Johannes', 200, 'Germany'),\n    ('Paul', 75, 'Poland'),\n    ('Bob', 35, 'Mexico'),\n    ('James', 21, 'Germany')\n]\n\n","user":"anonymous","dateUpdated":"2018-08-27T21:18:50+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1535404705344_-1469890999","id":"20180824-160727_1105840180","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:50+0000","dateFinished":"2018-08-27T21:19:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:255"},{"text":"case class Transactions(name:String, amount:Long, country:String)\n\nval transactions = List(\n    Transactions(\"Bob\", 25, \"MX\"),\n    Transactions(\"James\", 15, \"United Kingdom\"),\n    Transactions(\"Marek\", 51, \"Poland\"),\n    Transactions(\"Johannes\", 200,\"Germany\"),\n    Transactions(\"Paul\", 75, \"Poland\"),\n    Transactions(\"Bob\", 35, \"Mexico\"),\n    Transactions(\"James\", 21, \"Germany\")\n)","user":"anonymous","dateUpdated":"2018-08-27T21:18:50+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Transactions\ntransactions: List[Transactions] = List(Transactions(Bob,25,MX), Transactions(James,15,United Kingdom), Transactions(Marek,51,Poland), Transactions(Johannes,200,Germany), Transactions(Paul,75,Poland), Transactions(Bob,35,Mexico), Transactions(James,21,Germany))\n"}]},"apps":[],"jobName":"paragraph_1535404705349_-1471814744","id":"20180827-172331_1658203493","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:54+0000","dateFinished":"2018-08-27T21:19:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:256"},{"text":"%md\n\nThe following code snippet creates a data frame and outputs you the number of partitions, partitioner and partition structure.\n","user":"anonymous","dateUpdated":"2018-08-27T21:18:53+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The following code snippet creates a data frame and outputs you the number of partitions, partitioner and partition structure.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705350_-1470660497","id":"20180824-161222_1775895785","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:54+0000","dateFinished":"2018-08-27T21:18:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:257"},{"text":"%pyspark\nfrom pyspark.sql import SparkSession, Row\n    \ndf = spark.createDataFrame(transactions, TransactionSchema)\n\nprint(\"Number of partitions: {}\".format(df.rdd.getNumPartitions()))\nprint(\"Partitioner: {}\".format(df.rdd.partitioner))\nprint(\"Partitions structure: {}\".format(df.rdd.glom().collect()))","user":"anonymous","dateUpdated":"2018-08-27T21:18:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Number of partitions: 3\nPartitioner: None\nPartitions structure: [[Row(name=u'Bob', amount=100, country=u'United Kingdom'), Row(name=u'James', amount=15, country=u'United Kingdom')], [Row(name=u'Marek', amount=51, country=u'Poland'), Row(name=u'Johannes', amount=200, country=u'Germany')], [Row(name=u'Paul', amount=75, country=u'Poland'), Row(name=u'Bob', amount=35, country=u'Mexico'), Row(name=u'James', amount=21, country=u'Germany')]]\n"}]},"apps":[],"jobName":"paragraph_1535404705351_-1471045246","id":"20180824-161021_1826354548","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:54+0000","dateFinished":"2018-08-27T21:19:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:258"},{"text":"import org.apache.spark.sql.Row\n\n\nval df = spark.createDataFrame(transactions)\n\ndf.rdd.getNumPartitions\ndf.rdd.partitioner\ndf.rdd.glom().collect()\n","user":"anonymous","dateUpdated":"2018-08-27T21:18:54+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.Row\ndf: org.apache.spark.sql.DataFrame = [name: string, amount: bigint ... 1 more field]\nres4: Int = 3\nres5: Option[org.apache.spark.Partitioner] = None\nres6: Array[Array[org.apache.spark.sql.Row]] = Array(Array([Bob,25,MX], [James,15,United Kingdom]), Array([Marek,51,Poland], [Johannes,200,Germany]), Array([Paul,75,Poland], [Bob,35,Mexico], [James,21,Germany]))\n"}]},"apps":[],"jobName":"paragraph_1535404705351_-1471045246","id":"20180827-182233_1620123781","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:33+0000","dateFinished":"2018-08-27T21:19:46+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:259"},{"text":"%md\nLook at the output and answer the following questions:\n    - Names on partition 1\n    - Amounts on partition 2\n    - Why is the partitioned `None`?\n\n","user":"anonymous","dateUpdated":"2018-08-27T21:18:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Look at the output and answer the following questions:<br/> - Names on partition 1<br/> - Amounts on partition 2<br/> - Why is the partitioned <code>None</code>?</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705353_-1473353739","id":"20180824-161209_564012877","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:55+0000","dateFinished":"2018-08-27T21:18:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:260"},{"text":"%md\n**REPARTITIONING:** The following code repartitions the original DataFrame and outputs the same information. ","user":"anonymous","dateUpdated":"2018-08-27T21:18:55+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>REPARTITIONING:</strong> The following code repartitions the original DataFrame and outputs the same information.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705353_-1473353739","id":"20180824-161118_605198725","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:55+0000","dateFinished":"2018-08-27T21:18:55+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:261"},{"text":"%pyspark\ndf2 = df.repartition(\"country\")\n    \nprint(\"\\nAfter 'repartition()'\")\nprint(\"Number of partitions: {}\".format(df2.rdd.getNumPartitions()))\nprint(\"Partitioner: {}\".format(df2.rdd.partitioner))\nprint(\"Partitions structure: {}\".format(df2.rdd.glom().collect()))","user":"anonymous","dateUpdated":"2018-08-27T21:18:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nAfter 'repartition()'\nNumber of partitions: 200\nPartitioner: None\nPartitions structure: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(name=u'Johannes', amount=200, country=u'Germany'), Row(name=u'James', amount=21, country=u'Germany')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(name=u'Bob', amount=35, country=u'Mexico')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(name=u'Marek', amount=51, country=u'Poland'), Row(name=u'Paul', amount=75, country=u'Poland')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(name=u'Bob', amount=100, country=u'United Kingdom'), Row(name=u'James', amount=15, country=u'United Kingdom')], [], [], [], []]\n"}]},"apps":[],"jobName":"paragraph_1535404705354_-1472199493","id":"20180824-161919_1765365187","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:34+0000","dateFinished":"2018-08-27T21:19:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:262"},{"text":"val df2 = df.repartition($\"country\")\n\ndf2.rdd.getNumPartitions\ndf2.rdd.partitioner\ndf2.rdd.glom().collect()","user":"anonymous","dateUpdated":"2018-08-27T21:18:56+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{"0":{"graph":{"mode":"table","height":125,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [name: string, amount: bigint ... 1 more field]\nres8: Int = 200\nres9: Option[org.apache.spark.Partitioner] = None\nres10: Array[Array[org.apache.spark.sql.Row]] = Array(Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array([Bob,25,MX]), Array(), Array(), Array(), Array(), Array([Johannes,200,Germany], [James,21,Germany]), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(..."}]},"apps":[],"jobName":"paragraph_1535404705354_-1472199493","id":"20180827-191139_22040351","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:44+0000","dateFinished":"2018-08-27T21:19:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:263"},{"text":"%md\nLook at the output and answer the following questions:\n - Amounts on partition 1\n - Names on partition 2\n - What is the main difference?\n","user":"anonymous","dateUpdated":"2018-08-27T21:18:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Look at the output and answer the following questions:<br/> - Amounts on partition 1<br/> - Names on partition 2<br/> - What is the main difference?</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705354_-1472199493","id":"20180824-161944_2121074393","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:56+0000","dateFinished":"2018-08-27T21:18:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:264"},{"text":"%md\nRemember more partitions != uniform partition distribution. So more partitions is not necessarily the answer, in fact if you end up having too many partitions you may bump into a problem with very small files.","user":"anonymous","dateUpdated":"2018-08-27T21:18:56+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Remember more partitions != uniform partition distribution. So more partitions is not necessarily the answer, in fact if you end up having too many partitions you may bump into a problem with very small files.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705355_-1472584241","id":"20180824-163234_318434487","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:56+0000","dateFinished":"2018-08-27T21:18:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:265"},{"text":"%md\n# Exercise 2 - Partitioning approaches\n\nUsing the same dataset, create new datasets, try partitioning using each one of the columns. This means you will end with 4 distinct datasets.\n","user":"anonymous","dateUpdated":"2018-08-27T21:19:50+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Exercise 2 - Partitioning approaches</h1>\n<p>Using the same dataset, create new datasets, try partitioning using each one of the columns. This means you will end with 4 distinct datasets.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705355_-1472584241","id":"20180824-163253_580541449","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:57+0000","dateFinished":"2018-08-27T21:18:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:266"},{"text":"%pyspark\ndf2 = df.repartition(\"name\")\n    \nprint(\"\\nAfter 'repartition()' with column 'name'\")\nprint(\"Number of partitions: {}\".format(df2.rdd.getNumPartitions()))\nprint(\"Partitioner: {}\".format(df2.rdd.partitioner))\nprint(\"Partitions structure: {}\".format(df2.rdd.glom().collect()))\n","user":"anonymous","dateUpdated":"2018-08-27T21:18:57+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nAfter 'repartition()' with column 'name'\nNumber of partitions: 200\nPartitioner: None\nPartitions structure: [[], [], [], [], [], [], [], [], [], [], [], [], [Row(name=u'James', amount=21, country=u'Germany'), Row(name=u'James', amount=15, country=u'United Kingdom')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(name=u'Bob', amount=100, country=u'United Kingdom'), Row(name=u'Bob', amount=35, country=u'Mexico')], [], [], [Row(name=u'Marek', amount=51, country=u'Poland')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(name=u'Johannes', amount=200, country=u'Germany')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [Row(name=u'Paul', amount=75, country=u'Poland')], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"}]},"apps":[],"jobName":"paragraph_1535404705355_-1472584241","id":"20180824-163321_450981945","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:47+0000","dateFinished":"2018-08-27T21:19:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:267"},{"text":"val df2 = df.repartition($\"name\")\n\ndf2.rdd.getNumPartitions\ndf2.rdd.partitioner\ndf2.rdd.glom().collect()\n","user":"anonymous","dateUpdated":"2018-08-27T21:18:57+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [name: string, amount: bigint ... 1 more field]\nres12: Int = 200\nres13: Option[org.apache.spark.Partitioner] = None\nres14: Array[Array[org.apache.spark.sql.Row]] = Array(Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array([James,21,Germany], [James,15,United Kingdom]), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array(), Array..."}]},"apps":[],"jobName":"paragraph_1535404705355_-1472584241","id":"20180827-192209_233833264","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:50+0000","dateFinished":"2018-08-27T21:19:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:268"},{"text":"%md\nAfter printing the number of partitions, partitioner and partitions structure, answer the following question: \n - In this case, what is the best approach?\n - How can you determine that?\n","user":"anonymous","dateUpdated":"2018-08-27T21:18:57+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>After printing the number of partitions, partitioner and partitions structure, answer the following question:<br/> - In this case, what is the best approach?<br/> - How can you determine that?</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705356_-1474507986","id":"20180824-163507_1092132855","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:57+0000","dateFinished":"2018-08-27T21:18:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:269"},{"text":"%md\nAs you can see, the repartitioning has a direct effect on the distribution of the information, using the correct column is essential. For knowing the most appropriate column, you need to be familiar with the data, there is no workaround. ","user":"anonymous","dateUpdated":"2018-08-27T21:18:57+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>As you can see, the repartitioning has a direct effect on the distribution of the information, using the correct column is essential. For knowing the most appropriate column, you need to be familiar with the data, there is no workaround.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705356_-1474507986","id":"20180824-163550_767573207","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:58+0000","dateFinished":"2018-08-27T21:18:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:270"},{"text":"%md\n# Exercise 3 - I/O with single and multiple files\n\nNow, look at reading and writing. You already know this is a costly operation. Regardless, there are strategies for troubleshooting or improving the reading and writing performance. This exercise requires you to Read and Write a DataSet in a single file and multiple files to compare performance.\n\nQuestion: How do you determine the best approach?\nNote: unless you control what is being written, you can only make recommendations on the file sizes for you to read later. If you are given one big block then your fate is sealed.\n","user":"anonymous","dateUpdated":"2018-08-27T21:18:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Exercise 3 - I/O with single and multiple files</h1>\n<p>Now, look at reading and writing. You already know this is a costly operation. Regardless, there are strategies for troubleshooting or improving the reading and writing performance. This exercise requires you to Read and Write a DataSet in a single file and multiple files to compare performance.</p>\n<p>Question: How do you determine the best approach?<br/>Note: unless you control what is being written, you can only make recommendations on the file sizes for you to read later. If you are given one big block then your fate is sealed.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705356_-1474507986","id":"20180824-163615_1064210426","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:58+0000","dateFinished":"2018-08-27T21:18:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:271"},{"text":"%md First let's read the data. The dataset is quite big, so let's take a small sample instead:","user":"anonymous","dateUpdated":"2018-08-27T21:18:58+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>First let&rsquo;s read the data. The dataset is quite big, so let&rsquo;s take a small sample instead:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705357_-1474892735","id":"20180827-165943_350742125","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:58+0000","dateFinished":"2018-08-27T21:18:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:272"},{"text":"%pyspark\ndf = spark.read.json(\"gs://de-training-input/alimazon/50000/client-orders/*.gz\") \\\n    .sample(False, 0.2)  # take 20% of the data without replacement\ndf.count()","user":"anonymous","dateUpdated":"2018-08-27T21:18:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"13398\n"}]},"apps":[],"jobName":"paragraph_1535404705357_-1474892735","id":"20180827-172725_2084099563","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:53+0000","dateFinished":"2018-08-27T21:20:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:273"},{"text":"val df = spark.read.json(\"gs://de-training-input/alimazon/50000/client-orders/*.gz\")\n    .sample(false, 0.2) // take 20% of the data without replacement\ndf.count()","user":"anonymous","dateUpdated":"2018-08-27T21:18:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [client_id: string, id: string ... 4 more fields]\nres15: Long = 13373\n"}]},"apps":[],"jobName":"paragraph_1535404705357_-1474892735","id":"20180827-170048_1570856632","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:57+0000","dateFinished":"2018-08-27T21:20:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:274"},{"text":"%md **`NOTE:`** Do not forget to set you username here","user":"anonymous","dateUpdated":"2018-08-27T21:18:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong><code>NOTE:</code></strong> Do not forget to set you username here</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705358_-1473738488","id":"20180827-171324_1624649057","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:18:59+0000","dateFinished":"2018-08-27T21:18:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:275"},{"text":"%pyspark\nusername = \"<user-name>\"","user":"anonymous","dateUpdated":"2018-08-27T21:18:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1535404705360_-1463735017","id":"20180827-173600_941913882","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:58+0000","dateFinished":"2018-08-27T21:20:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:276"},{"text":"val username = \"<user-name>\"","user":"anonymous","dateUpdated":"2018-08-27T21:18:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"username: String = <user-name>\n"}]},"apps":[],"jobName":"paragraph_1535404705361_-1464119766","id":"20180827-171640_353868380","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:20:11+0000","dateFinished":"2018-08-27T21:20:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:277"},{"text":"%md Let's write our data using different partitions.\nConfirm in GCP that the dataset was written in the number of specified partitions.","user":"anonymous","dateUpdated":"2018-08-27T21:18:59+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Let&rsquo;s write our data using different partitions.<br/>Confirm in GCP that the dataset was written in the number of specified partitions.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705361_-1464119766","id":"20180827-170049_2139323805","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:00+0000","dateFinished":"2018-08-27T21:19:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:278"},{"text":"%pyspark\ndf.repartition(1).write.json(\"gs://de-training-output-{}/client-orders-1part/\".format(username))\ndf.repartition(6).write.json(\"gs://de-training-output-{}/client-orders-6part/\".format(username))","user":"anonymous","dateUpdated":"2018-08-27T21:19:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8720817955999194847.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8720817955999194847.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/spark/python/pyspark/sql/readwriter.py\", line 665, in json\n    self._jwrite.json(path)\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 79, in deco\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\nIllegalArgumentException: u'Invalid bucket name (de-training-output-<user-name>) or object name ()'\n\n"}]},"apps":[],"jobName":"paragraph_1535404705361_-1464119766","id":"20180827-171201_1462543897","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:20:12+0000","dateFinished":"2018-08-27T21:20:12+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:279"},{"text":"df.repartition(1).write.json(s\"gs://de-training-output-$username/client-orders-1part/\")\ndf.repartition(6).write.json(s\"gs://de-training-output-$username/client-orders-6part/\")\n","user":"anonymous","dateUpdated":"2018-08-27T21:19:00+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.IllegalArgumentException: Invalid bucket name (de-training-output-<user-name>) or object name ()\n  at com.google.cloud.hadoop.gcsio.LegacyPathCodec.getPath(LegacyPathCodec.java:99)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.configureBuckets(GoogleHadoopFileSystem.java:75)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:2011)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1102)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1065)\n  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2812)\n  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100)\n  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849)\n  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831)\n  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)\n  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)\n  at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:394)\n  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:471)\n  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:50)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n  at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:487)\n  ... 46 elided\nCaused by: java.net.URISyntaxException: Illegal character in authority at index 5: gs://de-training-output-<user-name>/\n  at java.net.URI$Parser.fail(URI.java:2848)\n  at java.net.URI$Parser.parseAuthority(URI.java:3186)\n  at java.net.URI$Parser.parseHierarchical(URI.java:3097)\n  at java.net.URI$Parser.parse(URI.java:3053)\n  at java.net.URI.<init>(URI.java:588)\n  at com.google.cloud.hadoop.gcsio.LegacyPathCodec.getPath(LegacyPathCodec.java:93)\n  ... 74 more\n"}]},"apps":[],"jobName":"paragraph_1535404705362_-1462965519","id":"20180827-173626_223417974","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:20:12+0000","dateFinished":"2018-08-27T21:20:14+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:280"},{"text":"%md Let's experiment a bit more reading and writing using different numbers of partitions:","user":"anonymous","dateUpdated":"2018-08-27T21:19:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Let&rsquo;s experiment a bit more reading and writing using different numbers of partitions:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705364_-1465274012","id":"20180827-192903_1248488032","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:00+0000","dateFinished":"2018-08-27T21:19:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:281"},{"text":"%md Here is the example for reading from one file and writing to one file","user":"anonymous","dateUpdated":"2018-08-27T21:19:00+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535404705365_-1465658761","id":"20180827-171740_251328243","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:01+0000","dateFinished":"2018-08-27T21:19:01+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:282"},{"text":"%pyspark\ndf1 = sqlContext.read.json(\"gs://de-training-output-{}/client-orders-1part/\".format(username))\ndf1.repartition(1).write.csv(\"gs://de-training-output-{}/client-orders-1in-1out/\".format(username))\n","user":"anonymous","dateUpdated":"2018-08-27T21:19:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8720817955999194847.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8720817955999194847.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/spark/python/pyspark/sql/readwriter.py\", line 249, in json\n    return self._df(self._jreader.json(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 79, in deco\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\nIllegalArgumentException: u'Invalid bucket name (de-training-output-<user-name>) or object name ()'\n\n"}]},"apps":[],"jobName":"paragraph_1535404705365_-1465658761","id":"20180827-173824_1583828117","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:20:12+0000","dateFinished":"2018-08-27T21:20:13+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:283"},{"text":"val df1 = spark.read.json(s\"gs://de-training-output-$username/client-orders-1part/\")\ndf1.repartition(1).write.csv(s\"gs://de-training-output-$username/client-orders-1in-1out/\")","user":"anonymous","dateUpdated":"2018-08-27T21:19:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.IllegalArgumentException: Invalid bucket name (de-training-output-<user-name>) or object name ()\n  at com.google.cloud.hadoop.gcsio.LegacyPathCodec.getPath(LegacyPathCodec.java:99)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.configureBuckets(GoogleHadoopFileSystem.java:75)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:2011)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1102)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1065)\n  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2812)\n  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100)\n  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849)\n  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831)\n  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)\n  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)\n  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:616)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n  at scala.collection.immutable.List.flatMap(List.scala:344)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:349)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:333)\n  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:279)\n  ... 46 elided\nCaused by: java.net.URISyntaxException: Illegal character in authority at index 5: gs://de-training-output-<user-name>/\n  at java.net.URI$Parser.fail(URI.java:2848)\n  at java.net.URI$Parser.parseAuthority(URI.java:3186)\n  at java.net.URI$Parser.parseHierarchical(URI.java:3097)\n  at java.net.URI$Parser.parse(URI.java:3053)\n  at java.net.URI.<init>(URI.java:588)\n  at com.google.cloud.hadoop.gcsio.LegacyPathCodec.getPath(LegacyPathCodec.java:93)\n  ... 68 more\n"}]},"apps":[],"jobName":"paragraph_1535404705365_-1465658761","id":"20180827-171820_27449283","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:20:13+0000","dateFinished":"2018-08-27T21:20:15+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:284"},{"text":"%md Here is the example for reading from multiple files and writing to multiple files","user":"anonymous","dateUpdated":"2018-08-27T21:19:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Here is the example for reading from multiple files and writing to multiple files</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705366_-1464504515","id":"20180827-171903_233529514","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:01+0000","dateFinished":"2018-08-27T21:19:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:285"},{"text":"%pyspark\ndf6 = sqlContext.read.json(\"gs://de-training-output-{}/client-orders-6part/\".format(username))\ndf6.repartition(6).write.csv(\"gs://de-training-output-{}/client-orders-6in-6out/\".format(username))","user":"anonymous","dateUpdated":"2018-08-27T21:19:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8720817955999194847.py\", line 367, in <module>\n    raise Exception(traceback.format_exc())\nException: Traceback (most recent call last):\n  File \"/tmp/zeppelin_pyspark-8720817955999194847.py\", line 355, in <module>\n    exec(code, _zcUserQueryNameSpace)\n  File \"<stdin>\", line 1, in <module>\n  File \"/usr/lib/spark/python/pyspark/sql/readwriter.py\", line 249, in json\n    return self._df(self._jreader.json(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n  File \"/usr/lib/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\", line 1133, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 79, in deco\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\nIllegalArgumentException: u'Invalid bucket name (de-training-output-<user-name>) or object name ()'\n\n"}]},"apps":[],"jobName":"paragraph_1535404705366_-1464504515","id":"20180827-174103_2146116059","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:20:14+0000","dateFinished":"2018-08-27T21:20:14+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:286"},{"text":"val df6 = spark.read.json(s\"gs://de-training-output-$username/client-orders-6part/\")\ndf6.repartition(6).write.csv(s\"gs://de-training-output-$username/client-orders-6in-6out/\")","user":"anonymous","dateUpdated":"2018-08-27T21:19:01+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"java.lang.IllegalArgumentException: Invalid bucket name (de-training-output-<user-name>) or object name ()\n  at com.google.cloud.hadoop.gcsio.LegacyPathCodec.getPath(LegacyPathCodec.java:99)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.configureBuckets(GoogleHadoopFileSystem.java:75)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:2011)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1102)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1065)\n  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2812)\n  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100)\n  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849)\n  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831)\n  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)\n  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)\n  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:616)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$14.apply(DataSource.scala:350)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n  at scala.collection.immutable.List.flatMap(List.scala:344)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:349)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:333)\n  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:279)\n  ... 46 elided\nCaused by: java.net.URISyntaxException: Illegal character in authority at index 5: gs://de-training-output-<user-name>/\n  at java.net.URI$Parser.fail(URI.java:2848)\n  at java.net.URI$Parser.parseAuthority(URI.java:3186)\n  at java.net.URI$Parser.parseHierarchical(URI.java:3097)\n  at java.net.URI$Parser.parse(URI.java:3053)\n  at java.net.URI.<init>(URI.java:588)\n  at com.google.cloud.hadoop.gcsio.LegacyPathCodec.getPath(LegacyPathCodec.java:93)\n  ... 68 more\n"}]},"apps":[],"jobName":"paragraph_1535404705366_-1464504515","id":"20180827-172046_737696731","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:20:14+0000","dateFinished":"2018-08-27T21:20:16+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:287"},{"text":"%md Explore using different number of partitions, reading from 1 file and writing to 4","user":"anonymous","dateUpdated":"2018-08-27T21:19:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Explore using different number of partitions, reading from 1 file and writing to 4</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705367_-1464889263","id":"20180827-172131_750013194","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:02+0000","dateFinished":"2018-08-27T21:19:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:288"},{"text":"%md\n# Exercise 4 - Flights dataset partitions size\n\nThe next code help you to display the size of each partition, we are going to generate a sample dataset\n","user":"anonymous","dateUpdated":"2018-08-27T21:19:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Exercise 4 - Flights dataset partitions size</h1>\n<p>The next code help you to display the size of each partition, we are going to generate a sample dataset</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705368_-1466813008","id":"20180824-163816_107227786","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:02+0000","dateFinished":"2018-08-27T21:19:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:289"},{"text":"%pyspark\nfrom itertools import chain\nfrom pyspark.sql import Row\n\nsample_dataset = spark.createDataFrame(chain([(i, 0) for i in range(1, 100001)], [(i, j) for i in range(1, 101) for j in range(1, 101)])).cache()\nprint(sample_dataset.rdd.getNumPartitions())\npartitions = sample_dataset.rdd.mapPartitions(lambda part: [Row(value=len(list(part)))])\nspark.createDataFrame(partitions).show()","user":"anonymous","dateUpdated":"2018-08-27T21:54:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"3\n+-----+\n|value|\n+-----+\n|36864|\n|36864|\n|36272|\n+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1535404705368_-1466813008","id":"20180827-174300_1370563640","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:54:18+0000","dateFinished":"2018-08-27T21:54:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:290"},{"text":"val sampleDataSet = ((1 to 100000).map((_,0))++(1 to 100).flatMap(i => (1 to 100).map((_,i)))).toDS.cache\nprintln(sampleDataSet.rdd.getNumPartitions)\nval partitions = sampleDataSet.mapPartitions(iter => Seq(iter.size).iterator)\npartitions.show()","user":"anonymous","dateUpdated":"2018-08-27T21:53:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"sampleDataSet: org.apache.spark.sql.Dataset[(Int, Int)] = [_1: int, _2: int]\n3\npartitions: org.apache.spark.sql.Dataset[Int] = [value: int]\n+-----+\n|value|\n+-----+\n|36666|\n|36667|\n|36667|\n+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1535404705371_-1466428259","id":"20180824-163856_1912329826","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:53:41+0000","dateFinished":"2018-08-27T21:53:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:291"},{"text":"%md\nNow, using your acquired knowledge of former sessions, load the flights DataSet and display the size of the partitions\n","user":"anonymous","dateUpdated":"2018-08-27T21:19:02+0000","config":{"tableHide":true,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now, using your acquired knowledge of former sessions, load the flights DataSet and display the size of the partitions</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1535404705371_-1466428259","id":"20180824-163919_2039405814","dateCreated":"2018-08-27T21:18:25+0000","dateStarted":"2018-08-27T21:19:02+0000","dateFinished":"2018-08-27T21:19:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:292"},{"text":"","user":"anonymous","dateUpdated":"2018-08-27T21:19:02+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1535404705372_-1468352004","id":"20180824-163954_1813225578","dateCreated":"2018-08-27T21:18:25+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:293"}],"name":"Spark Data Partitioning","id":"2DP8SFKJG","angularObjects":{"2DQQSHGRZ:shared_process":[],"2DQJ1WCHK:shared_process":[],"2DP1QG92N:shared_process":[],"2DP6K2Y5P:shared_process":[],"2DRNS4UGC:shared_process":[],"2DQ2PDHW1:shared_process":[],"2DRWSPJCR:shared_process":[],"2DRMRDUAE:shared_process":[],"2DP8EPWZE:shared_process":[],"2DPD3VD8K:shared_process":[],"2DPMZQQ1S:shared_process":[],"2DPRZJGCU:shared_process":[],"2DQ5MKTDH:shared_process":[],"2DQ8PFAR2:shared_process":[],"2DQ8H35E1:shared_process":[],"2DPJNWKZ1:shared_process":[],"2DNVYDW2P:shared_process":[],"2DRBE954Z:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}